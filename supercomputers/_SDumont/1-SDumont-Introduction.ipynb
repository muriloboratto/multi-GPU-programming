{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c007fdcd",
   "metadata": {},
   "source": [
    "# GPU Programming Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa0d53",
   "metadata": {},
   "source": [
    "Welcome to the webinar _Introduction the CUDA-Aware MPI in Santos Dummont Summer School 2026 Program. In this webinar you will learn several techniques for scaling single GPU applications to multi-GPU and multiple nodes, with an emphasis on [NCCL (NVIDIA Collective Communications Library)](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html), [CUDA-Aware MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/), and [NVSHMEM](https://developer.nvidia.com/nvshmem) which allows for elegant multi-GPU application code and has been proven to scale very well on systems with many GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087395a",
   "metadata": {},
   "source": [
    "## The Coding Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755c507",
   "metadata": {},
   "source": [
    "The first step is display information about the CPU architecture with the command `lscpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cddea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f3558",
   "metadata": {},
   "source": [
    "In this node, we can observe that the multi-GPU resources connect with the NUMA nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686add1",
   "metadata": {},
   "source": [
    "For your work today, you have access to several GPUs in the cloud. Run the following cell to see the GPUs available to you today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901503da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04314c13",
   "metadata": {},
   "source": [
    "While your work today will be on a single node, all the techniques you learn today, in particular CUDA-Aware MPI and NVSHMEM, can be used to run your applications across clusters of multi-GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e24d1",
   "metadata": {},
   "source": [
    "Let us show the NVLink Status for different GPUs reported from `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi nvlink --status -i 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf551095",
   "metadata": {},
   "source": [
    "In the end, it gives information about the NUMA memory nodes, with tue `lstopo` command, that is used to show the topology of the system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstopo --of png > sdumont.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c96a34-0e22-4767-b2f5-728ce100c1da",
   "metadata": {},
   "source": [
    "This will import and display a .png image in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac6f5a-849b-44ff-8422-4b36c653303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "path=\"sdumont.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75b087-d76e-47a6-aa8c-528257ebd7f7",
   "metadata": {},
   "source": [
    "## Environment Modules on SDUMONT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e7bb8-ae38-49ea-b222-8402444dcbfb",
   "metadata": {},
   "source": [
    "These modules must be initialized before running the jupyter-notebook:\n",
    "```\n",
    "module load openmpi/gnu/4.1.4+gcc-12.4+cuda-11.6_sequana\n",
    "\n",
    "Currently Loaded Modulefiles:\n",
    "  1) xpmem/2.7.4_gcc-12.4_sequana                \n",
    "  2) cuda/12.6_sequana                        \n",
    "  3) ucx/1.17_gcc-12.4+cuda-12.6_sequana                    \n",
    "  4) hwloc/2.11.2_gcc-12.4_sequana\n",
    "  5) libevent/2.1.12_gcc-12.4_sequana\n",
    "  6) gcc/12.4.0_sequana\n",
    "  7) libltdl/2.5.3_gcc-12.4_sequana\n",
    "  8) openpmix/4.2.8_sequana\n",
    "  9) openmpi/gnu/4.1.4+gcc-12.4+cuda-11.6_sequana\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea3fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2677eed",
   "metadata": {},
   "source": [
    "During this short course today you will work through each of the following notebooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baba471",
   "metadata": {},
   "source": [
    "- [_NCCL_](2-MC-SD03-II-SDumont-NCCL-P2P.ipynb): In this notebook you will introduced the NCCL API, and the concepts of peer-to-peer communication between GPUs.\n",
    "\n",
    "- [_CUDA-Aware MPI_](3-MC-SD03-II-SDumont-CUDAWARE-MPI.ipynb): You will begin by familiarizing with the concepts of CUDA-Aware MPI API to multi-GPU nodes.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Single GPU_](4-MC-SD03-II-SDumont-MCπ-SGPU.ipynb): You will begin by familiarizing yourself with a single GPU implementation of the monte-carlo approximation of π algorithm, which we will use to introduce many multi GPU programming paradigms.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs_](5-MC-SD03-II-SDumont-MCπ-MGPU.ipynb): In this notebook you will extend the monte-carlo π program to run on multiple GPUs by looping over available GPU devices.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - CUDA-Aware MPI_](6-MC-SD03-II-SDumont-MCπ-CUDAWARE-MPI): In this notebook you will learn how to applied the CUDA-Aware MPI, and some concepts about peer-to-peer communication between GPUs in the SPMD paradigm.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM_](7-MC-SD03-II-SDumont-MCπ-NVSHMEM.ipynb): In this notebook you will be introduced to NVSHMEM, and will take your first pass with it using the monte-carlo π program.\n",
    "\n",
    "- [_Jacobi Iteration_](8-MC-SD03-II-SDumont-Jacobi.ipynb): In this notebook you will be introduced to a Laplace equation solver using Jacobi iteration and will learn how to use NVSHMEM to handle boundary communications between multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67cdca-e78d-44b2-9f7f-cd00f17585eb",
   "metadata": {},
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aacc10-80c9-43b9-bf2a-10a0b373f6fd",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45734bed-66b4-42ca-a7c9-80e0bea620a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1952987-b6fe-4029-8959-69795ccd792f",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa7a69-59a4-41bd-9110-519ad6ecdf16",
   "metadata": {},
   "source": [
    "In the next section, you will see  how to learn the GPU concepts using the API NCCL [_2-SDumont-NCCL-P2P.ipynb_](2-SDumont-NCCL-P2P.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
